{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b990916-898c-4521-a92a-a7bb39399d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "from fastapi import FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945caff4-7894-436c-8f76-a7c653789253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.197.153:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug: * Restarting with stat\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aleksandra.rancic/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aleksandra.rancic/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"/Users/aleksandra.rancic/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 118, in inner\n",
      "    return method(app, *args, **kwargs)\n",
      "  File \"/Users/aleksandra.rancic/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 692, in initialize\n",
      "    self.init_sockets()\n",
      "  File \"/Users/aleksandra.rancic/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 331, in init_sockets\n",
      "    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n",
      "  File \"/Users/aleksandra.rancic/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 253, in _bind_socket\n",
      "    return self._try_bind_socket(s, port)\n",
      "  File \"/Users/aleksandra.rancic/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 229, in _try_bind_socket\n",
      "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
      "  File \"/Users/aleksandra.rancic/Library/Python/3.9/lib/python/site-packages/zmq/sugar/socket.py\", line 311, in bind\n",
      "    super().bind(addr)\n",
      "  File \"_zmq.py\", line 898, in zmq.backend.cython._zmq.Socket.bind\n",
      "  File \"_zmq.py\", line 160, in zmq.backend.cython._zmq._check_rc\n",
      "zmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:64839')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "class model_input(BaseModel):\n",
    "    Date : object\n",
    "    Open : float\n",
    "    High : float\n",
    "    Low : float\n",
    "    Close : float\n",
    "    Adj Close : float\n",
    "    Volume : int\n",
    "\n",
    "# Define folder path to store datasets\n",
    "folder_path = \"/Users/aleksandra.rancic/Desktop/MLOps/dataset\"\n",
    "\n",
    "# Load the trained LSTM model and the MinMaxScaler\n",
    "model = pickle.load(open('lstm_model.pkl'))\n",
    "scaler = joblib.load('minmax_scaler.pkl')\n",
    "\n",
    "@app.post('/stock_prediction')\n",
    "def stock_pred(input_parameters : model_input):\n",
    "    input_data = input_parameters.json()\n",
    "    input_dictionary = json.loads(input_data)\n",
    "    Date = input_dictionary['']\n",
    "    Open = input_dictionary['']\n",
    "    High = input_dictionary['']\n",
    "    Low = \n",
    "# Function to download the dataset\n",
    "def download_dataset(label, start_date):\n",
    "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    stock_data = yf.download(label, start=start_date, end=end_date)\n",
    "    file_name = f\"{label}_stock_data.csv\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    stock_data.to_csv(file_path)\n",
    "    return stock_data\n",
    "\n",
    "# Load evaluation metrics from the JSON file\n",
    "def load_evaluation_metrics():\n",
    "    with open('lstm_model_metrics.json', 'r') as file:\n",
    "        metrics = json.load(file)\n",
    "    return metrics\n",
    "\n",
    "# Endpoint to check API status\n",
    "@app.route('/status', methods=['GET'])\n",
    "def status():\n",
    "    return jsonify({\"status\": \"API is running\"}), 200\n",
    "\n",
    "# Endpoint to download stock data and preprocess\n",
    "@app.route('/download', methods=['POST'])\n",
    "def download_stock_data():\n",
    "    data = request.get_json(force=True)\n",
    "    label = data.get('label', 'AAPL')\n",
    "    start_date = data.get('start_date', '2014-09-12')\n",
    "    \n",
    "    # Download stock data\n",
    "    stock_data = download_dataset(label, start_date)\n",
    "    \n",
    "    return jsonify({\"message\": f\"Stock data for {label} downloaded successfully!\"}), 200\n",
    "\n",
    "# Endpoint to predict stock prices\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    \n",
    "    # Get the input features from the request\n",
    "    input_data = np.array(data['features']).reshape(-1, 1)\n",
    "\n",
    "    # Scale the input data using the loaded scaler\n",
    "    scaled_input = scaler.transform(input_data)\n",
    "\n",
    "    # Prepare the input data for LSTM\n",
    "    sequence_length = 60\n",
    "    x_input = []\n",
    "    for i in range(len(scaled_input) - sequence_length):\n",
    "        x_input.append(scaled_input[i:i + sequence_length])\n",
    "\n",
    "    x_input = np.array(x_input)\n",
    "\n",
    "    # Make predictions using the LSTM model\n",
    "    predictions = model.predict(x_input)\n",
    "\n",
    "    # Inverse transform the predictions to get the actual stock prices\n",
    "    predicted_values = scaler.inverse_transform(predictions)\n",
    "\n",
    "    return jsonify({\"prediction\": predicted_values.tolist()}), 200\n",
    "\n",
    "# Endpoint to get the model evaluation metrics\n",
    "@app.route('/evaluation', methods=['GET'])\n",
    "def evaluation():\n",
    "    metrics = load_evaluation_metrics()\n",
    "    return jsonify(metrics), 200\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116d349-60f1-4d01-9685-0c801558b662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
